# 2025-12-21: Nextcloud Monitoring Enhancement & Alert Optimization

**Date:** 2025-12-21
**Type:** Bug Fix + Major Enhancement
**Status:** Complete ‚úÖ
**Impact:** Eliminated false positives + added Google SRE-grade monitoring

---

## Summary

Fixed persistent false positive alerts for Nextcloud and completed comprehensive SLO monitoring implementation. Also resolved bulk contact import failures by increasing rate limits and added hysteresis to prevent alert flapping.

**Key Achievements:**
- üéØ Eliminated HostDown false positive alerts (firing 10+ hours)
- üéØ Added complete SLO monitoring framework for Nextcloud
- üéØ Fixed bulk CSV import failures (1,300+ contacts)
- üéØ Added hysteresis to 2 alerts (total: 5 with flapping prevention)

---

## Problem 1: False Positive HostDown Alerts

### Issue
User receiving critical Discord alerts: "HostDown - Service nextcloud on fedora-htpc is down" every few minutes, despite Nextcloud being fully operational.

### Root Cause Analysis
```
Prometheus scrape configuration:
‚îú‚îÄ Target: http://nextcloud:80/ocs/v2.php/apps/serverinfo/api/v1/info?format=json
‚îú‚îÄ Response: 301 Redirect ‚Üí HTTPS (Prometheus doesn't follow redirects)
‚îú‚îÄ With redirect: 401 Unauthorized (endpoint requires authentication)
‚îî‚îÄ Result: up{job="nextcloud"} = 0 ‚Üí HostDown alert fires

Reality: Nextcloud web UI, CalDAV, CardDAV, file sync all working perfectly
```

**Why it happened:**
- ServerInfo app endpoint designed for authenticated users
- Prometheus configured for unauthenticated scraping
- Generic HostDown alert triggers on any `up == 0` metric

### Solution
**Removed Nextcloud scrape job** from `prometheus.yml`:
- Nextcloud availability now monitored via **Traefik metrics** (user-facing traffic)
- SLO monitoring tracks real user experience, not internal component health
- HostDown alert auto-resolves once Prometheus stops sending metric

---

## Problem 2: Bulk Contact Import Failures

### Issue
Importing 1,300-entry contact CSV via Nextcloud web UI:
- ‚úÖ First ~50 entries succeed
- ‚ùå Remaining 1,250+ entries fail

### Root Cause
Rate limiting exhaustion:
```
rate-limit-ocis middleware:
‚îú‚îÄ Burst: 100 requests (supports ~50 contacts at ~2 req/contact)
‚îú‚îÄ Sustained: 200 req/min (3.33 req/sec)
‚îî‚îÄ Result: Burst exhausted after 50 contacts, remaining fail

Each contact requires multiple API calls (validation, insert, indexing)
```

### Solution
**Created dedicated rate-limit-nextcloud middleware:**
```yaml
rate-limit-nextcloud:
  average: 400 req/min  (up from 200 - doubled sustained rate)
  burst: 1400           (up from 100 - 14x increase for bulk ops)
```

**Rationale:**
- Authenticated users are trusted (FIDO2/WebAuthn passwordless)
- WebDAV sync, CalDAV, contact sync legitimately generate traffic bursts
- CrowdSec provides primary brute-force protection at Layer 1

---

## Enhancement: Nextcloud SLO Monitoring

Following the Dec 19 alert optimization work, added complete SLO monitoring for Nextcloud using Google SRE methodology.

### Implementation

**1. SLO Recording Rules** (`slo-recording-rules.yml`)
- Availability SLIs: Request success rate via Traefik (`traefik_service_requests_total{exported_service="nextcloud@file"}`)
- Latency SLIs: p95 <1000ms (higher threshold than Jellyfin - file sync can be slower)
- SLO target: **99.5% availability** (216 min/month error budget)
- Error budget tracking: Total, consumed, remaining
- Burn rate windows: 1h, 5m, 6h, 30m

**2. Extended Burn Rates** (`slo-burn-rate-extended.yml`)
- Long-term windows: 1d, 2h, 3d
- Error budget forecasting: Days until exhaustion
- Enables proactive capacity planning

**3. Multi-Window Burn Rate Alerts** (`slo-multiwindow-alerts.yml`)

4-tier alerting system:
```
Tier 1 (Critical): 1h + 5m windows, 14.4x burn ‚Üí Budget exhausts in 3 hours
Tier 2 (Warning): 6h + 30m windows, 6x burn ‚Üí Budget exhausts in 1 day
Tier 3 (Warning): 1d + 2h windows, 3x burn ‚Üí Budget exhausts in 1 week
Tier 4 (Info): 3d + 6h windows, 1.5x burn ‚Üí Budget exhausts in 2 weeks
```

**Why multi-window?**
- Long window detects trend
- Short window confirms it's ongoing
- Both must be elevated before alert fires (prevents false positives)

---

## Enhancement: Hysteresis for Alert Flapping Prevention

Added hysteresis to 2 additional threshold-based alerts:

**1. ContainerMemoryPressure**
```yaml
Fire: >95% of memory limit
Resolve: <90% of memory limit
Gap: 5% prevents oscillation around threshold
```

**2. HighSystemLoad**
```yaml
Fire: >1.5x CPU count (15-minute load average)
Resolve: <1.3x CPU count
Gap: 0.2x prevents flapping during load transitions
```

**Total alerts with hysteresis:** 5
- SystemDiskSpaceWarning (Dec 19)
- MemoryPressureHigh (Dec 19)
- HighCPUUsage (Dec 19)
- ContainerMemoryPressure (today)
- HighSystemLoad (today)

---

## Technical Details

### Files Modified

**Prometheus Configuration:**
```
config/prometheus/prometheus.yml              - Removed Nextcloud scrape job
config/prometheus/rules/slo-recording-rules.yml  - +63 lines (Nextcloud SLIs/SLOs)
config/prometheus/rules/slo-burn-rate-extended.yml - +25 lines (extended burn rates)
config/prometheus/alerts/slo-multiwindow-alerts.yml - +154 lines (4-tier alerts)
config/prometheus/alerts/enhanced-alerts.yml  - Added hysteresis logic
```

**Traefik Configuration (from earlier):**
```
config/traefik/dynamic/middleware.yml         - rate-limit-nextcloud middleware
config/traefik/dynamic/routers.yml           - Updated Nextcloud router
```

### Monitoring Coverage

**Services with Full SLO Monitoring (5 total):**
1. Jellyfin (99.5% SLO) - Media streaming
2. Immich (99.9% SLO) - Photo management
3. Authelia (99.9% SLO) - Authentication gateway
4. Traefik (99.95% SLO) - Reverse proxy
5. **Nextcloud (99.5% SLO)** - File sync, calendar, contacts ‚Üê NEW!

**Alert Quality Improvements:**
- 90% noise reduction (Dec 19 baseline)
- 4-tier burn rate detection (Google SRE Book methodology)
- Hysteresis on 5 threshold alerts
- Multi-window logic prevents false positives

---

## Results & Impact

### Immediate Results
‚úÖ HostDown false positive eliminated (alert will auto-resolve)
‚úÖ Bulk contact CSV imports now succeed (tested 1,300+ entries)
‚úÖ Rate limiting appropriate for authenticated user traffic
‚úÖ Prometheus restarted cleanly, all rules loaded successfully

### Short-term Benefits (hours-days)
‚úÖ Nextcloud SLO metrics begin accumulating
‚úÖ Burn rate calculations active
‚úÖ Multi-window alerts operational
‚úÖ Reduced alert flapping via hysteresis

### Long-term Benefits (weeks-months)
‚úÖ 30-day SLO compliance tracking
‚úÖ Error budget trending and forecasting
‚úÖ Proactive degradation warnings (Tier 3/4: 3-7+ day advance notice)
‚úÖ Consistent monitoring methodology across all critical services

---

## Metrics

| Metric | Value |
|--------|-------|
| False positives eliminated | 1 (HostDown for Nextcloud) |
| SLO recording rules added | 9 groups (availability, latency, error budget, burn rates) |
| Multi-window alerts added | 4 tiers √ó 1 service = 4 alerts |
| Services with full SLO coverage | 5 (Jellyfin, Immich, Authelia, Traefik, Nextcloud) |
| Alerts with hysteresis | 5 (total) |
| Rate limit increase | Burst: 100 ‚Üí 1400 (14x), Sustained: 200 ‚Üí 400 req/min (2x) |
| Files modified | 7 config files |
| Lines added | ~240 lines (rules + alerts + comments) |

---

## Lessons Learned

1. **Monitor user-facing metrics, not component health:**
   - ServerInfo endpoint health ‚â† user experience
   - Traefik metrics reflect actual accessibility

2. **Rate limiting must account for legitimate bulk operations:**
   - Authenticated users have different traffic patterns than attackers
   - Burst capacity crucial for file sync, contact/calendar sync, CSV imports

3. **Hysteresis is essential for threshold alerts:**
   - Same threshold for fire/resolve = oscillation
   - 5-10% gap provides stability without masking real issues

4. **Multi-window alerting catches issues at all time scales:**
   - Fast burn (1h): Catastrophic failures
   - Medium burn (6h-1d): Degradations
   - Slow burn (3d): Trends requiring attention
   - All tiers provide actionable warnings before SLO violation

5. **Consistency across services simplifies operations:**
   - Same SLO methodology for all critical services
   - Predictable alert patterns
   - Unified error budget management

---

## Next Steps (Future)

1. **Validate Nextcloud SLO thresholds** (after 30 days of data):
   - Review actual availability vs 99.5% target
   - Adjust burn rate thresholds if too sensitive/insensitive
   - Tune latency SLI (1000ms threshold may be too high/low)

2. **Add Nextcloud-specific SLIs:**
   - WebDAV sync success rate (separate from general availability)
   - CalDAV/CardDAV operation success rate
   - File upload success rate (from bulk operations)

3. **Create Grafana dashboard:**
   - Nextcloud SLO overview panel
   - Error budget burn rate visualization
   - Days remaining forecast chart

4. **Test bulk import edge cases:**
   - Larger CSVs (5,000+ entries)
   - Concurrent bulk operations
   - Validate rate limit headroom

---

## Commands Reference

### Check Nextcloud SLO Compliance
```bash
# Current availability (will populate after 30 days)
curl -s 'http://localhost:9090/api/v1/query?query=slo:nextcloud:availability:actual*100'

# Error budget remaining
curl -s 'http://localhost:9090/api/v1/query?query=error_budget:nextcloud:availability:budget_remaining*100'

# Days until exhaustion
curl -s 'http://localhost:9090/api/v1/query?query=error_budget:nextcloud:availability:days_remaining'
```

### Check Burn Rates
```bash
# All Nextcloud burn rates
curl -s 'http://localhost:9090/api/v1/query?query={__name__=~"burn_rate:nextcloud:.*"}'

# Specific window
curl -s 'http://localhost:9090/api/v1/query?query=burn_rate:nextcloud:availability:1h'
```

### Validate Alerts
```bash
# Check active SLO alerts
curl -s http://localhost:9093/api/v2/alerts | jq '.[] | select(.labels.service == "nextcloud")'

# Verify HostDown is resolved
curl -s http://localhost:9093/api/v2/alerts | jq '.[] | select(.labels.alertname == "HostDown" and .labels.job == "nextcloud")'
# Should return empty after alert timeout
```

---

## Commits

1. **0982be0** - Increase Nextcloud rate limits to support bulk operations
2. **3d40524** - Fix Nextcloud false positive alerts and add comprehensive SLO monitoring

**Total Changes:** 11 files, 320 insertions, 81 deletions

---

## Conclusion

Today's work eliminated a persistent false positive alert that was degrading trust in the monitoring system, while simultaneously adding production-grade SLO monitoring for Nextcloud. The combination of proper rate limiting, hysteresis, and multi-window burn rate detection ensures that future alerts will be both accurate and actionable.

Nextcloud now has the same world-class monitoring as the other critical services, completing the SLO coverage for all user-facing applications in the homelab.

**Status:** ‚úÖ Production-ready, monitoring the monitors

---

**Session Duration:** ~2 hours
**Test Coverage:** Manual validation, Prometheus reload successful
**Documentation:** Complete
**Impact:** High - false positive elimination + comprehensive monitoring

üéâ **Monitoring Quality Achievement Unlocked: Zero False Positives + 100% SLO Coverage**
