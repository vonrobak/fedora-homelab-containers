# Backup Compression Strategy Evaluation

**Date:** 2026-02-02
**Author:** patriark (with Claude Sonnet 4.5)
**Context:** External backup drive critical capacity (89% full, 1.9TB/17TB free)
**Objective:** Evaluate backup improvement proposals focusing on compression and efficiency
**Status:** ‚úÖ Phase 1 Complete - Compression enabled via fstab

---

## Executive Summary

Current backup system is **fundamentally sound but storage-inefficient**. Critical finding: **Source filesystems use zstd:1 compression, but destination had NO compression**.

**Key Discovery:** Using `btrfs filesystem du` revealed that apparent sizes (from `du -sh`) were massively inflated due to snapshot deduplication. Actual storage usage is 11.2TB, not 29.5TB.

**Implementation Complete:** Destination filesystem compression (zstd:3) enabled via fstab. New backups will be compressed. Existing 11.2TB of uncompressed data can be retroactively compressed to reclaim **0.9-1.2TB** (extends drive life 6-8 months).

**Next Steps:** Monitor compression effectiveness for 2-4 weeks, then optionally force-recompress existing snapshots.

---

## Current State Analysis

### System Configuration

**Source Filesystems** (All using `compress=zstd:1`):
- `/` (nvme0n1p3): System root, zstd level 1 compression
- `/home` (nvme0n1p3): User data, zstd level 1 compression
- `/mnt/btrfs-pool` (/dev/sdc): 10.5TB used, zstd level 1 compression

**Destination Filesystem** (WD-18TB external):
- `/run/media/patriark/WD-18TB`: ~~NO compression~~ **NOW ENABLED (zstd:3)** ‚úÖ
- Current usage: 15TB/17TB (89% full, 1.9TB free)
- LUKS encrypted, BTRFS filesystem
- Always connected (internal SATA limitation workaround)
- Managed via fstab (not removable)

**BTRFS Version:** v6.17.1 (supports zstd, --compressed-data flag)
**CPU Resources:** 12 cores available (excellent for parallel compression/defragmentation)

### Accurate Storage Breakdown (via btrfs filesystem du)

**CRITICAL:** Standard `du -sh` severely over-counts due to snapshot deduplication. Using `btrfs filesystem du -s` for accuracy:

| Subvolume | Apparent (du) | **Actual (Excl+Shared)** | Snapshots | Dedup Ratio |
|-----------|---------------|--------------------------|-----------|-------------|
| opptak | 20.59 TiB | **3.0 TiB** | 7 weekly | 85% shared |
| multimedia | 5.89 TiB | **5.89 TiB** | 1 | 0% (disabled) |
| music | 2.01 TiB | **2.01 TiB** | 1 | 0.5% |
| home | 219 GiB | **68 GiB** | ~8 | 69% shared |
| pics | 128 GiB | **128 GiB** | 1 | 0% |
| tmp | 93 GiB | **93 GiB** | 1 | 0% |
| docs | 80 GiB | **23 GiB** | ~8 | 71% shared |
| containers | 39 GiB | **19 GiB** | ~4 | 51% shared |
| root | 27 GiB | **27 GiB** | 1 | 1.3% |
| **TOTAL** | **29.5 TiB** | **~11.2 TiB** | ‚Äî | **62% avg** |

**Key Insights:**
- **opptak deduplication is exceptional:** 534 MiB of changes across 7 weeks (99.98% stable)
- **Media dominates:** 10.9 TiB of 11.2 TiB (97%) is pre-compressed video/audio
- **Compressible data:** Only ~360 GiB (docs, configs, databases)
- **df shows 15TB used:** Difference is metadata, free space fragmentation, filesystem overhead

### The Compression Gap

**Problem:** When `btrfs send` runs on a compressed source and `btrfs receive` writes to an uncompressed destination, data is **decompressed during transfer** and written uncompressed.

**Revised Impact Assessment:**

Given that 97% of storage is pre-compressed media (video/audio formats), compression gains will be modest:

**Media (opptak 3TB + multimedia 5.89TB + music 2.01TB = 10.9TB):**
- Already compressed (H.264/H.265 video, AAC/MP3 audio)
- BTRFS compression on pre-compressed data: 5-10% typical
- **Estimated savings: 545 GiB - 1.09 TiB**

**Compressible data (docs/home/containers/root/pics/tmp = ~360 GiB):**
- Text files, configs, database indexes, some images
- BTRFS zstd:3 compression: 30-40% typical
- **Estimated savings: 108-144 GiB**

**Total conservative estimate: 0.65-1.23 TiB (900 GiB median)**

**More realistic than initial estimate** (3-5TB was too optimistic, didn't account for media dominance)

### Backup Schedule & Retention

| Tier | Subvolume | Local Freq | External Freq | Ext. Retention | Observed Snapshots |
|------|-----------|------------|---------------|----------------|-------------------|
| 1 | htpc-home | Daily | Weekly (Sat) | 8w + 12m | ~8 |
| 1 | opptak | Daily | Weekly (Sat) | 8w + 12m | **7** ‚ö†Ô∏è |
| 1 | containers | Daily | Weekly (Sat) | 4w + 6m | ~4 |
| 2 | docs | Daily | Weekly (Sat) | 8w + 6m | ~8 |
| 2 | root | Monthly | Monthly | 6m | 1 |
| 3 | pics | Weekly | Monthly | 12m | 1 |
| 3 | multimedia | Weekly | **DISABLED** | 6m | 1 (local only) |
| 3 | music | Weekly | Monthly | 6m | 1 |
| 3 | tmp | Weekly | Monthly | 3m | 1 |

**Note:** opptak shows only 7 snapshots (Dec 20 ‚Üí Jan 31), not seeing separate "monthly" retention category. May indicate retention logic needs review (future investigation).

---

## Top 5 Proposals Evaluated

### Proposal #1: Stream-to-Archive Compression ‚ùå REJECTED

**Approach:** Compress btrfs send streams to .zst archives instead of live receive.

```bash
# Send compressed archive
sudo btrfs send /snapshot | zstd -3 -T12 > /external/snapshot.btrfs.zst

# Later restore
zstd -d /external/snapshot.btrfs.zst | sudo btrfs receive /restore/location
```

**Pros:**
- Maximum compression control (adjustable zstd levels 1-19)
- Can parallelize compression across 12 cores (-T12)
- Lower disk I/O (write compressed file vs. btrfs subvolume metadata)
- Enables off-site transfer (can rsync .zst files)
- Industry standard for cold archival (Borg, restic use similar approach)

**Cons:**
- ‚ùå **Major**: Snapshots not instantly browsable (must decompress first)
- ‚ùå **Major**: Two-step restore process (decompress, then receive)
- ‚ùå **Major**: Breaks incremental send/receive workflow (each archive independent)
- ‚ùå Additional script complexity (manage .zst files, rotation, verification)
- ‚ùå User-unfriendliness: Can't browse snapshot to grab single file
- ‚ö†Ô∏è Compression time overhead (~10-20% longer backup window)

**Space Efficiency:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (best compression, 40-60% on compressible data)
**Performance:** ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (slower backups, slower restores)
**User-Friendliness:** ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ (major usability regression)
**Recovery Simplicity:** ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ (two-step restore, must track .zst files)
**Industry Alignment:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (Borg/restic use this, different use case)

**Verdict:** ‚ùå **REJECTED** - Sacrifices too much usability. Breaks "just grab the file from snapshot" workflow that makes BTRFS snapshots valuable. Better suited for cold archival than active backup rotation.

---

### Proposal #2: Destination Filesystem Compression ‚úÖ IMPLEMENTED

**Approach:** Enable transparent compression on external drive filesystem.

**Implementation Method:** Fstab entry (drive is always connected, not removable)

```bash
# /etc/fstab entry
UUID=b6b38ff1-4f6d-4fd3-8e76-d9857ffce224 /run/media/patriark/WD-18TB btrfs compress=zstd:3,nofail,nosuid,nodev 0 0
```

**Why fstab, not udev:**
- ‚úÖ Drive always connected (SATA port limitation workaround)
- ‚úÖ Consistent with existing setup (main pool also in fstab)
- ‚úÖ More reliable than udev/udisks2 automount
- ‚úÖ Boots with correct options guaranteed
- ‚úÖ Simpler debugging (standard mount commands)

**Technical Details:**
- Compression applies to new data written via `btrfs receive`
- Existing uncompressed data remains uncompressed (not retroactive)
- Can force recompression with `btrfs filesystem defragment -czstd`
- Zstd level 3 balances compression ratio vs. speed (Fedora default)

**Pros:**
- ‚úÖ **Major**: Zero script changes required (transparent to backup script)
- ‚úÖ **Major**: Snapshots remain instantly browsable (no decompression step)
- ‚úÖ **Major**: Incremental send/receive continues to work normally
- ‚úÖ Simple implementation (single mount option)
- ‚úÖ User-friendly: No workflow changes
- ‚úÖ Compatible with existing retention/cleanup logic
- ‚úÖ Industry best practice (Synology, TrueNAS use filesystem compression)
- ‚úÖ Can adjust compression level per subvolume if needed

**Cons:**
- ‚ö†Ô∏è Existing 11.2TB data remains uncompressed (gradual conversion via defrag)
- ‚ö†Ô∏è Small CPU overhead during writes (~5-10% with zstd:3, negligible on 12-core)
- ‚ö†Ô∏è Defragmentation CPU/I/O intensive (addressed in Phase 3)

**Space Efficiency:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (0.9-1.2TB savings, gradual with new backups)
**Performance:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (minimal overhead, may improve I/O)
**User-Friendliness:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (completely transparent)
**Recovery Simplicity:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (no changes to restore procedure)
**Industry Alignment:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (NAS vendors, cloud providers standard practice)

**Implementation Status:** ‚úÖ **COMPLETE**

```bash
# Verified mount options (2026-02-02)
/dev/mapper/luks-b6b38ff1-4f6d-4fd3-8e76-d9857ffce224 on /run/media/patriark/WD-18TB
type btrfs (rw,nosuid,nodev,relatime,seclabel,compress=zstd:3,space_cache=v2,subvolid=5,subvol=/)
```

**Expected Impact:**
- **Immediate:** New backups ~7-10% smaller (media-dominated)
- **6 months:** ~300-400GB reclaimed (as old snapshots rotate out)
- **12 months:** ~500-700GB total savings (without retroactive compression)
- **With retroactive compression:** 0.9-1.2TB total (see Phase 3)

**Verdict:** ‚úÖ **IMPLEMENTED** - Industry-standard approach. Zero risk, high reward, maximum user-friendliness.

---

### Proposal #3: Increased External Backup Frequency ‚öôÔ∏è DEFERRED

**Approach:** Change Tier 1 external backups from weekly to 2-3x per week.

**Current:** Saturday only (weekly)
**Proposed:** Tuesday, Thursday, Saturday (3x weekly) OR Tuesday, Saturday (2x weekly)

**Rationale:**
- Incremental sends are cheap (only changed blocks sent)
- Weekly interval risks larger data loss window (up to 7 days)
- Multimedia excluded (5.8TB), so most backups are small
- Current critical data (home/opptak/containers/docs) ~240GB + incremental changes

**Pros:**
- ‚úÖ Reduces potential data loss window (7 days ‚Üí 2-3 days)
- ‚úÖ Incremental sends are fast (opptak: 534MB/7weeks = 76MB/week avg)
- ‚úÖ Better aligns with "critical" tier designation
- ‚úÖ More frequent verification of backup health
- ‚úÖ Spreads I/O load across week (vs. Saturday spike)

**Cons:**
- ‚ö†Ô∏è Slightly more storage (more frequent snapshots)
- ‚ö†Ô∏è Increased wear on external drive (more frequent writes)
- ‚ö†Ô∏è More complex scheduling (day-of-week logic)
- ‚ö†Ô∏è May complicate "weekly" vs "monthly" retention distinction

**Space Efficiency:** ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ (slightly worse - more snapshots)
**Performance:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (incremental sends are fast)
**User-Friendliness:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (transparent if automated)
**Recovery Simplicity:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (no change to restore)
**Industry Alignment:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (enterprise often does daily incrementals)

**Implementation:**

```bash
# Modify backup functions (e.g., backup_tier1_home in script)
# BEFORE: if [[ $(date +%u) -eq 6 ]]; then  # Saturday only
# AFTER:
if [[ $(date +%u) =~ ^(2|4|6)$ ]]; then  # Tue, Thu, Sat
    check_external_mounted || return 1
    # ... send incremental backup
fi
```

**Estimated Impact:**
- **RPO improvement:** 7 days ‚Üí 2-3 days (weekly ‚Üí 3x weekly)
- **Storage increase:** ~50-100GB (2 additional snapshots √ó 7 tiers)
- **Backup window:** +5-10 minutes per additional backup day

**Verdict:** ‚öôÔ∏è **DEFERRED** - Good optimization after confirming compression savings. Implement if:
1. After 6 months, drive has >15% free space (compression successful)
2. User wants shorter RPO for critical data
3. Not needed if retroactive compression reclaims sufficient space

---

### Proposal #4: Aggressive Retention Reduction ‚ö†Ô∏è POSTPONED

**Approach:** Reduce external retention counts to free immediate space.

**Current External Retention:**
```
Tier 1: 8 weekly + 12 monthly (opptak/home)
Tier 1: 4 weekly + 6 monthly (containers)
Tier 2: 8 weekly + 6 monthly (docs)
Tier 3: 12 monthly (pics), 6 monthly (multimedia/music), 3 monthly (tmp)
```

**Proposed Reductions (if needed):**
```
Tier 1 opptak: 4 weekly + 3 monthly (recordings less valuable over time)
Tier 1 home/docs/containers: Keep current (compressible, irreplaceable)
Tier 3 media: Reduce monthly retention by 50%
```

**Pros:**
- ‚úÖ Immediate space savings (1.5-2TB for opptak alone)
- ‚úÖ Simple implementation (just change retention variables)
- ‚úÖ No performance impact
- ‚úÖ Reduces backup rotation overhead
- ‚úÖ Reasonable for time-sensitive data (video recordings)

**Cons:**
- ‚ùå Shorter recovery history (less safety margin)
- ‚ùå Permanent data loss (old snapshots deleted)
- ‚ùå Not addressing root cause (inefficient storage)
- ‚ö†Ô∏è Need to verify retention is actually working (opptak has 7 snapshots, not 8+12)

**Space Efficiency:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (immediate 1.5-2TB for opptak)
**Performance:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (no impact)
**User-Friendliness:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (transparent)
**Recovery Simplicity:** ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ (less history = fewer recovery options)
**Industry Alignment:** ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ (industry trend is MORE retention, not less)

**Verdict:** ‚ö†Ô∏è **POSTPONED** - User decision deferred. Retroactive compression preferred as primary space reclamation strategy (non-destructive). Can revisit if:
1. Compression fails to provide sufficient space
2. External drive <5% free
3. Temporary measure until drive upgrade

---

### Proposal #5: Tiered Compression Strategy ‚öôÔ∏è FUTURE

**Approach:** Use different compression levels for different data tiers.

**Proposed Configuration:**
```
Tier 1 critical (docs, home, containers): zstd:5 (higher compression)
Tier 2 important: zstd:3 (balanced - current default)
Tier 3 media (opptak, multimedia, music): zstd:1 (fast, minimal benefit)
```

**Implementation:** Per-subvolume compression on destination
```bash
# After receiving snapshot, adjust compression property
sudo btrfs property set /external/.snapshots/opptak/20260207-opptak compression zstd:1
sudo btrfs property set /external/.snapshots/docs/20260207-docs compression zstd:5

# Force recompress at new level
sudo btrfs filesystem defragment -czstd /path/to/snapshot
```

**Pros:**
- ‚úÖ Optimizes CPU/space tradeoff per data value
- ‚úÖ Higher compression for irreplaceable data (docs, configs)
- ‚úÖ Lower overhead for low-compressibility data (video/audio)
- ‚úÖ Sophisticated approach (shows architectural maturity)

**Cons:**
- ‚ö†Ô∏è Complex implementation (post-receive property setting + defrag)
- ‚ö†Ô∏è Must track which subvolumes have which compression level
- ‚ö†Ô∏è Marginal benefit (zstd:1 vs zstd:3 only ~3-5% difference on media)
- ‚ö†Ô∏è May cause confusion during recovery ("why is this snapshot smaller?")
- ‚ö†Ô∏è Requires testing compression levels per data type

**Space Efficiency:** ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (incremental 50-100GB over uniform zstd:3)
**Performance:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (optimized per tier)
**User-Friendliness:** ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ (complexity hidden but discoverable)
**Recovery Simplicity:** ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (transparent during restore)
**Industry Alignment:** ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (sophisticated backup systems, not common in homelab)

**Verdict:** ‚öôÔ∏è **FUTURE OPTIMIZATION** - Implement ONLY after:
1. Baseline uniform compression (zstd:3) stable for 6+ months
2. Space pressure remains despite compression
3. User wants to optimize further (complexity-to-benefit ratio low)

Recommendation: Start with uniform zstd:3, gather data on which subvolumes consume most space, then tune if needed.

---

## Comparison Matrix

| Proposal | Space Savings | Complexity | User Impact | Recovery | Implementation | Status |
|----------|--------------|------------|-------------|----------|----------------|--------|
| #1: Stream Archive | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (max) | High | High (negative) | Medium | 4-8 hours | ‚ùå Rejected |
| #2: Filesystem Compress | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (0.9-1.2TB) | Very Low | None | None | 15 minutes | ‚úÖ Complete |
| #3: Increased Frequency | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ (slight decrease) | Medium | Low | None | 2-3 hours | ‚öôÔ∏è Deferred |
| #4: Reduced Retention | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (1.5-2TB) | Very Low | None | Medium (negative) | 10 minutes | ‚ö†Ô∏è Postponed |
| #5: Tiered Compression | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ (50-100GB) | High | Low | Low | 6-10 hours | ‚öôÔ∏è Future |

**Revised savings based on actual data:** Media-dominated storage (97%) means compression gains are modest but worthwhile.

---

## Implementation Roadmap

### Phase 1: Enable Compression ‚úÖ COMPLETE (2026-02-02)

**Implemented via fstab entry:**

```bash
# /etc/fstab
UUID=b6b38ff1-4f6d-4fd3-8e76-d9857ffce224 /run/media/patriark/WD-18TB btrfs compress=zstd:3,nofail,nosuid,nodev 0 0
```

**Verification:**
```bash
$ mount | grep WD-18TB
/dev/mapper/luks-b6b38ff1-4f6d-4fd3-8e76-d9857ffce224 on /run/media/patriark/WD-18TB
type btrfs (rw,nosuid,nodev,relatime,seclabel,compress=zstd:3,space_cache=v2,subvolid=5,subvol=/)
```

**Status:** ‚úÖ Compression active. Next backup (Saturday 04:00) will write compressed data.

**Expected Outcome:**
- New backups ~7-10% smaller (conservative, media-dominated)
- Zero script changes required
- Zero user workflow impact
- Gradual space reclamation as old snapshots rotate out

---

### Phase 2: Monitor & Measure (2-4 weeks) üîÑ IN PROGRESS

**Goals:**
1. Verify compression is working on new snapshots
2. Measure actual compression ratios
3. Estimate space reclamation timeline
4. Determine if retroactive compression needed

**Monitoring Commands:**

```bash
# Install compsize for compression analysis
sudo dnf install compsize

# Check compression ratio on new snapshots (run after next backup)
sudo compsize /run/media/patriark/WD-18TB/.snapshots/

# Example output interpretation:
# Processed 1000 files, 50 GiB (uncompressed)
# Total compressed size: 45 GiB
# Compression ratio: 1.11:1 (10% savings)

# Compare new snapshot size to historical average
du -sh /run/media/patriark/WD-18TB/.snapshots/subvol3-opptak/20260208-opptak  # New
du -sh /run/media/patriark/WD-18TB/.snapshots/subvol3-opptak/20260131-opptak  # Old

# Track free space weekly
df -h /run/media/patriark/WD-18TB | tee -a ~/containers/data/backup-logs/space-tracking.log

# Generate weekly report
echo "$(date): $(df -h /run/media/patriark/WD-18TB | tail -1)" >> ~/space-trend.log
```

**Success Criteria:**
- New snapshots showing compression in `compsize` output
- Compression ratio 1.05-1.15:1 for media-heavy backups (5-15% savings)
- Compression ratio 1.3-1.5:1 for compressible data like docs/configs (30-50% savings)
- No backup failures or errors
- Backup duration unchanged or faster (compression can reduce I/O)

**Timeline:** 2-4 weeks (capture 2-4 backup cycles across different tiers)

---

### Phase 3: Retroactive Compression (Optional - 2-4 weeks) üìã PLANNED

**Goal:** Force-recompress existing 11.2TB of uncompressed snapshots to reclaim 0.9-1.2TB immediately.

**Approach:** BTRFS defragmentation with compression flag

**‚ö†Ô∏è Important Considerations:**

1. **CPU/I/O Intensive:** Will utilize significant system resources
2. **Time Required:** ~20-40 hours for 11.2TB (varies by CPU, disk speed)
3. **Snapshot Growth:** Defragmentation breaks some COW sharing, may temporarily increase space usage
4. **Run During Idle Time:** Recommended to run overnight/weekends
5. **Monitor Progress:** Can take days, must ensure it completes

**Implementation Options:**

**Option A: Aggressive (all data at once)**
```bash
# WARNING: CPU/I/O intensive, may take 24-48 hours
# Recommended: Run in screen/tmux session
sudo nice -n 19 ionice -c 3 btrfs filesystem defragment -r -v -czstd \
  /run/media/patriark/WD-18TB/.snapshots/ \
  2>&1 | tee ~/containers/data/backup-logs/defrag-$(date +%Y%m%d).log

# Monitor progress
watch -n 300 'df -h /run/media/patriark/WD-18TB && compsize /run/media/patriark/WD-18TB/.snapshots/'
```

**Option B: Conservative (per-subvolume, prioritize compressible data)**
```bash
# Start with smallest, most compressible subvolumes
# Priority order: docs (23GB) ‚Üí containers (19GB) ‚Üí home (68GB)

# Docs (highest compression potential)
sudo nice -n 19 ionice -c 3 btrfs filesystem defragment -r -v -czstd \
  /run/media/patriark/WD-18TB/.snapshots/subvol1-docs/ \
  2>&1 | tee ~/defrag-docs-$(date +%Y%m%d).log

# Wait for completion, monitor space savings
compsize /run/media/patriark/WD-18TB/.snapshots/subvol1-docs/

# If successful and space savings good, continue with others
# Containers
sudo nice -n 19 ionice -c 3 btrfs filesystem defragment -r -v -czstd \
  /run/media/patriark/WD-18TB/.snapshots/subvol7-containers/

# Home
sudo nice -n 19 ionice -c 3 btrfs filesystem defragment -r -v -czstd \
  /run/media/patriark/WD-18TB/.snapshots/htpc-home/

# SKIP media initially (opptak, multimedia, music) - low compression potential
# Can defrag later if space still tight
```

**Option C: Smart (skip media entirely, focus on compressible)**
```bash
# Only defragment data with good compression potential
# Skip opptak (3TB video), multimedia (5.89TB), music (2.01TB)
# Total to compress: ~360GB instead of 11.2TB

for subvol in subvol1-docs subvol7-containers htpc-home htpc-root subvol2-pics subvol6-tmp; do
  echo "Defragmenting $subvol..."
  sudo nice -n 19 ionice -c 3 btrfs filesystem defragment -r -v -czstd \
    /run/media/patriark/WD-18TB/.snapshots/$subvol/ \
    2>&1 | tee ~/defrag-$subvol-$(date +%Y%m%d).log

  echo "Compression results for $subvol:"
  compsize /run/media/patriark/WD-18TB/.snapshots/$subvol/
  echo "---"
done

# Expected savings (Option C):
# 360GB √ó 35% avg compression = ~126GB reclaimed
# Much faster (hours vs days), lower risk
```

**Recommendation:** Start with **Option C (smart/selective)** because:
- ‚úÖ Low risk (only ~360GB to process, 2-4 hours)
- ‚úÖ Best compression ratios (compressible data only)
- ‚úÖ ~125GB savings with minimal effort
- ‚úÖ Skip low-value targets (media barely compresses)
- ‚úÖ Can always do media later if needed

**Risk Mitigation:**

```bash
# Before starting, verify free space is sufficient
# Defragmentation may temporarily increase space usage (breaking COW sharing)
df -h /run/media/patriark/WD-18TB
# Ensure at least 10% free (1.7TB) before starting

# Run in screen/tmux to survive disconnections
screen -S defrag
# ... run defrag commands ...
# Ctrl+A, D to detach
# screen -r defrag to reattach

# Monitor system resources during defrag
htop  # Check CPU usage
iotop  # Check I/O (may need: sudo dnf install iotop)
```

**When to Run:**
- Best: Weekend/overnight when system idle
- Good: After verifying Phase 2 compression is working
- Avoid: During backup windows (Saturday 04:00, daily 02:00)

**Expected Timeline:**
- Option C (selective): 2-4 hours, ~125GB savings
- Option B (per-subvolume): 8-16 hours spread over days, ~300-400GB savings
- Option A (all at once): 24-48 hours, ~900GB-1.2TB savings

**Decision Point:** After Phase 2 monitoring (2-4 weeks), evaluate:
- If space reclamation from new compressed backups is sufficient ‚Üí Skip Phase 3
- If drive still >85% full ‚Üí Implement Option C (selective defrag)
- If drive approaching 95% ‚Üí Implement Option A (aggressive defrag)

---

### Phase 4: Long-Term Optimization (4-6 months) üîÆ FUTURE

**IF space savings insufficient after Phases 1-3:**

**Option 1: Implement Proposal #3 (Increased Frequency)**
- Only if external drive permanently connected
- Implement for Tier 1 only (most critical)
- Monitor impact for 1 month before expanding
- Benefit: Shorter RPO (7 days ‚Üí 2-3 days)
- Cost: Slightly more storage (~50-100GB)

**Option 2: Evaluate Proposal #5 (Tiered Compression)**
- Analyze compsize output to identify low-ratio subvolumes
- Test zstd:5 on small critical subvolume (docs)
- Measure benefit vs. complexity
- Only implement if measurable gains (>100GB)

**Option 3: Selective Retention Reduction (Proposal #4)**
- Focus on opptak (time-sensitive recordings)
- Reduce to 4 weekly + 3 monthly (from 8w + 12m)
- **Potential savings: 1.5-2TB**
- Keep full retention for docs/home/containers

**Option 4: Plan Drive Upgrade**
- Target: 36TB drive (~$600-800 as of 2026)
- Current 18TB should last 24+ months with compression
- Budget for replacement in 2027-2028
- With compression: Extended to 2028-2029

---

## Additional Recommendations

### 1. Install compsize for compression monitoring ‚úÖ RECOMMENDED

```bash
sudo dnf install compsize

# Usage examples:
compsize /run/media/patriark/WD-18TB/.snapshots/                    # All snapshots
compsize /run/media/patriark/WD-18TB/.snapshots/subvol1-docs/       # Specific subvolume
compsize -x /run/media/patriark/WD-18TB/.snapshots/subvol3-opptak/  # Exclude snapshots within
```

**Sample Output:**
```
Processed 50000 files, 3.2 TiB (uncompressed size)
Compressed size: 3.0 TiB
Compression ratio: 1.07:1
Compression savings: 200 GiB (7%)
```

### 2. Add compression metrics to backup script (Future Enhancement)

```bash
# Add to export_prometheus_metrics() function (~line 956)
echo "# HELP backup_compression_ratio Compression ratio for snapshots"
echo "# TYPE backup_compression_ratio gauge"

# Requires parsing compsize output per subvolume
# Implementation deferred to future script enhancement
```

### 3. Document compression in backup strategy guide

After Phase 2 monitoring (2-4 weeks), update `docs/20-operations/guides/backup-strategy.md`:

```markdown
## Compression Strategy (Implemented 2026-02-02)

**Status:** Active on external backup drive

**Configuration:**
- Mount option: `compress=zstd:3` (balanced compression/speed)
- Applied to: All new backups via btrfs receive
- Retroactive: [Yes/No] - see journal 2026-02-02 for decision

**Measured Results:** (after 4 weeks)
- Compression ratio: [X.XX:1]
- Space savings: [XXX GB]
- Performance impact: [None/Minimal/Measurable]

**Recommendation:** [Keep as-is / Adjust level / Consider tiered approach]
```

### 4. Verify retention logic is working correctly

**Observation:** opptak shows only 7 snapshots, but configured for 8 weekly + 12 monthly.

**Investigate:**
```bash
# Check what cleanup_old_snapshots is doing
grep -A 20 "cleanup_old_snapshots.*opptak" ~/containers/scripts/btrfs-snapshot-backup.sh

# Manually verify snapshot ages
ls -lt /run/media/patriark/WD-18TB/.snapshots/subvol3-opptak/

# Expected: Some snapshots >8 weeks old (monthly retention)
# Actual: Oldest is Dec 20 (6 weeks ago as of Feb 2)
# Possible issue: Monthly retention not creating separate snapshots?
```

**Future:** Review retention logic, ensure "monthly" snapshots are kept beyond weekly window.

### 5. Consider multimedia external backup (after compression proven)

**Currently:** `TIER3_MULTIMEDIA_EXTERNAL_ENABLED=false` (5.8TB)

**After compression + defrag reclaims 1-1.5TB:**
```bash
# External drive will have ~3-3.5TB free
# 5.8TB multimedia compressed at ~7% = 5.4TB needed
# Borderline feasible, but tight

# Decision: Wait until drive upgrade (36TB) OR
# Delete oldest multimedia snapshot to free 5.8TB local space
```

---

## Risk Assessment

### Low Risk ‚úÖ
- Proposal #2 (destination compression): Non-destructive, easily reversible, transparent
- Monitoring/measurement: Read-only operations
- Selective defragmentation (Option C): Small data set, low risk

### Medium Risk ‚ö†Ô∏è
- Proposal #3 (increased frequency): Can revert to weekly if issues
- Aggressive defragmentation (Option A): May temporarily increase space usage
- Proposal #5 (tiered compression): Complexity may cause maintenance burden

### High Risk ‚ùå
- Proposal #1 (stream archives): Breaks existing recovery procedures, hard to reverse
- Proposal #4 (reduced retention): Permanent data loss (old snapshots deleted)
- Full defragmentation during backup window: May cause backup failures

**Mitigation:**
- Start with Proposal #2 (lowest risk, implemented) ‚úÖ
- Monitor for 2-4 weeks before Phase 3
- Use selective defragmentation (Option C) if attempting Phase 3
- Never implement Proposal #4 without user confirmation
- Always run defragmentation during idle time, not backup windows

---

## Industry Best Practices Alignment

### What Enterprise/NAS Vendors Do

**Synology DSM:**
- Enables BTRFS compression by default (zstd or lz4)
- Provides UI toggle for compression level per shared folder
- Reports compression ratios in storage manager
- ‚úÖ **Aligns with Proposal #2** ‚Üê We implemented this

**TrueNAS:**
- Offers ZFS compression (lz4, zstd) at dataset level
- Tiered compression common (critical data = zstd:9, media = lz4)
- ‚úÖ Aligns with Proposal #2 + #5 (future)

**Borg Backup / Restic:**
- Compression in stream (zstd, lz4) - aligns with Proposal #1
- BUT: Designed for cold archival, not live snapshots
- Different use case than BTRFS send/receive

**Cloud Backup Services (Backblaze, AWS Glacier):**
- Automatic compression + deduplication
- Tiered storage (hot/warm/cold) with different compression levels
- ‚úÖ Aligns with Proposal #5 philosophy

**Veeam / Veritas NetBackup:**
- Incremental forever with compression
- Frequent incrementals (daily/hourly) - aligns with Proposal #3
- Synthetic fulls to avoid full backups
- ‚úÖ Aligns with current weekly incremental approach

**Conclusion:** Our implementation (Proposal #2 via fstab) is **industry-standard best practice** for NAS/homelab backup systems. Proposals #3 and #5 are enterprise-grade optimizations. Proposal #1 suited for cold archival, not active rotation.

---

## Lessons Learned

### 1. Always Use `btrfs filesystem du` for Snapshots

**Problem:** Standard `du -sh` reported 21TB for opptak (impossible on 18TB drive).

**Root Cause:** `du` counts shared extents multiple times across snapshots.

**Solution:** `btrfs filesystem du -s` shows actual disk usage:
- **Total:** Apparent size (what du shows)
- **Exclusive:** Unique to this subvolume
- **Set shared:** Shared within snapshots
- **Actual usage:** Exclusive + Set shared

**Takeaway:** Never trust `du` for BTRFS snapshots. Use `btrfs filesystem du` or `compsize`.

### 2. Media Dominates Storage, Compresses Poorly

**Finding:** 10.9 TiB of 11.2 TiB (97%) is pre-compressed video/audio.

**Impact:** Original estimate of 3-5TB savings was too optimistic (didn't account for media dominance).

**Revised:** 0.9-1.2TB realistic (7-10% compression on media, 30-40% on configs).

**Takeaway:** Analyze data composition before estimating compression gains. Pre-compressed formats (H.264, AAC, JPEG) barely compress further.

### 3. Fstab vs Udev Depends on Usage Pattern

**Initial assumption:** External drive ‚Üí use udev rules.

**Reality:** Drive always connected (SATA port workaround) ‚Üí fstab more appropriate.

**Decision factors:**
- Removable/hotplug ‚Üí udev
- Always connected ‚Üí fstab
- Consistent with existing setup ‚Üí fstab

**Takeaway:** Choose based on actual usage, not just "external" label.

### 4. Retroactive Compression Risky But Valuable

**Dilemma:** 11.2TB uncompressed data exists, gradual rotation takes 12+ months.

**Options:**
- Wait: 6-12 months for natural rotation
- Defrag: Force recompression (risky, CPU/I/O intensive)
- Selective: Defrag only compressible data (best balance)

**Recommendation:** Selective defragmentation (Option C) - 360GB in 2-4 hours for 125GB savings.

**Takeaway:** Defragmentation can accelerate benefits but must be done carefully (idle time, nice/ionice, monitor space during operation).

### 5. Retention Logic Needs Verification

**Observation:** opptak shows 7 snapshots, expected 8 weekly + monthly.

**Hypothesis:** Retention cleanup may not distinguish "monthly" category, or monthly retention hasn't triggered yet (first month).

**Action:** Future investigation of cleanup_old_snapshots logic.

**Takeaway:** Implemented retention != verified retention. Audit actual snapshot counts vs. configured retention.

---

## Conclusion

The backup system is **architecturally sound** and now **storage-efficient** with compression enabled.

**Key Achievements:**
1. ‚úÖ **Compression enabled** via fstab (Proposal #2) - industry best practice
2. ‚úÖ **Accurate storage analysis** via `btrfs filesystem du` - opptak is 3TB, not 21TB
3. ‚úÖ **Realistic expectations** - 0.9-1.2TB savings (not 3-5TB), media-dominated storage
4. ‚úÖ **Zero workflow disruption** - snapshots remain browsable, scripts unchanged
5. ‚úÖ **Monitoring plan** - compsize, space tracking, compression ratio verification

**Recommended Next Steps:**
1. **Phase 2 (2-4 weeks):** Monitor compression effectiveness on new backups
2. **Phase 3 (optional):** Selective defragmentation (360GB compressible data) for ~125GB immediate savings
3. **Phase 4 (4-6 months):** Re-evaluate if space still constrained, consider frequency increase or retention tuning

**Expected Outcomes:**
- **Short-term (3 months):** 300-400GB reclaimed via new compressed backups
- **Medium-term (6 months):** 500-700GB total (natural rotation)
- **With defragmentation:** 900GB-1.2TB total (immediate + ongoing)
- **Drive longevity:** Extended 6-8 months minimum, possibly 12+ months

The backup system now follows enterprise/NAS vendor best practices. Future optimizations (Proposals #3, #4, #5) available if needed, but compression alone should provide 6-12 months breathing room.

---

**Status:** ‚úÖ Phase 1 Complete - Ready for Phase 2 Monitoring
**Next Review Date:** 2026-03-02 (4 weeks, after 4-5 backup cycles)
**Confidence Level:** Very High (industry-standard approach, minimal risk, proven effectiveness)

