# SLO Operational Excellence: Phase 1-2 Diagnostic Findings

**Date:** 2026-01-22
**Status:** Phase 1-2 Complete - Critical Issues Identified
**Assessment Period:** January 15-22, 2026

## Executive Summary

**Services are currently healthy**, but the SLO remediation system has a critical architectural flaw that prevents proactive alerting. Snapshot collection has been restored, and error budgets are recovering from historical incidents.

### Key Findings

1. ‚úÖ **Services Currently Healthy** - Jellyfin 100%, Immich 97.3% success (24h)
2. üö® **CRITICAL BUG** - Burn rate calculation uses 30-day SLI instead of short-term windows
3. ‚úÖ **Snapshot Collection Restored** - Now capturing valid data after 12-day gap
4. üìà **Recovery in Progress** - Immich improved from 89.01% (Jan 9) to 95.66% (Jan 22)

---

## Phase 1: Current Service Health Assessment

### SLO Performance (30-day rolling window)

| Service | Current SLI | Target | Error Budget | Status | Trend |
|---------|-------------|--------|--------------|--------|-------|
| **Jellyfin** | 98.45% | 99.5% | -2.1 min | ‚ö†Ô∏è Non-compliant | Improving |
| **Immich** | 95.66% | 99.9% | -42.4 min | üö® Non-compliant | Recovering |
| **Authelia** | 99.78% | 99.9% | -1.2 min | ‚ö†Ô∏è Non-compliant | Stable |
| **Nextcloud** | 99.94% | 99.5% | +0.87 min | ‚úÖ Compliant | Stable |
| **Traefik** | 99.997% | 99.95% | +0.93 min | ‚úÖ Compliant | Excellent |

### Recent Performance (24-hour window)

**Jellyfin:**
- Total requests: 5
- Successful: 5 (100%)
- Error rate: **0.0%**
- Service logs: Non-critical errors only (audio normalization, metadata fetch)

**Immich:**
- Total requests: 633
- Successful: 616 (97.3%)
- Error rate: **2.7%** (~17 failed requests)
- Service logs: VAAPI encoding errors (non-blocking, retries with software encoding)

**Assessment:** Both services are performing well currently. The negative error budgets reflect **historical issues** (Immich corruption in early January, Jellyfin incidents in December), not current degradation.

### Burn Rate Analysis

| Service | 1h Burn | 6h Burn | Expected | Alert Status |
|---------|---------|---------|----------|--------------|
| Jellyfin | 3.11x | 3.11x | <6x for Tier 2 | ‚úÖ No alert |
| Immich | 43.36x | 43.40x | >14.4x for Tier 1 | ‚ùå **Should alert** |

**Critical Finding:** Despite Immich's 43x burn rate (well above the 14.4x Tier 1 threshold), **no alerts are firing**. Investigation revealed a fundamental flaw in the burn rate calculation.

---

## üö® CRITICAL: Burn Rate Calculation Bug

### The Problem

Burn rates are calculated based on the **30-day SLI** (historical average), not short-term performance windows:

```yaml
# Current implementation (INCORRECT)
- record: burn_rate:immich:availability:1h
  expr: |
    (1 - avg_over_time(sli:immich:availability:ratio[1h]))
    / error_budget:immich:availability:budget_total

# Where sli:immich:availability:ratio is ALIASED to:
- record: sli:immich:availability:ratio
  expr: slo:immich:availability:actual  # 30-day constant value (0.9566)
```

### Why This Breaks Alerting

1. **`sli:immich:availability:ratio`** = 0.9566 (30-day SLI, nearly constant)
2. **`avg_over_time(0.9566[1h])`** = 0.9566 (averaging a constant returns the constant)
3. **Burn rate** = `(1 - 0.9566) / 0.001` = **43.4x** (always reflects 30-day history)

This means:
- ‚úÖ Burn rate correctly shows long-term error budget consumption rate
- ‚ùå Burn rate does NOT detect short-term degradation (fast outages)
- ‚ùå Multi-window alerting CANNOT work as designed

### Expected Behavior

Burn rates should calculate **actual failure rate over the specified window**:

```yaml
# Correct implementation
- record: burn_rate:immich:availability:1h
  expr: |
    (
      1 - (
        sum(rate(traefik_service_requests_total{exported_service="immich@file", code=~"2..|3.."}[1h]))
        /
        sum(rate(traefik_service_requests_total{exported_service="immich@file"}[1h]))
      )
    ) / error_budget:immich:availability:budget_total
```

### Impact Assessment

**Tier 1 alerts (1h + 5m windows):** BROKEN - Won't fire during fast-burning incidents
**Tier 2 alerts (6h + 30m windows):** BROKEN - Won't detect medium-term degradation
**Tier 3/4 alerts (1d+ windows):** PARTIALLY WORKING - Detect sustained issues eventually
**Remediation webhook:** NEVER TRIGGERED - No Tier 1/2 alerts to route

**Severity:** üî¥ **CRITICAL** - Primary protection mechanism is non-functional

---

## Phase 2: Snapshot Collection Status

### Historical Gap

**Valid snapshots:** 2 collection points
- **Jan 9, 10:06** - Initial snapshot (5 services)
- **Jan 22, 00:13** - Manual run today (5 services)

**Null values:** 130 automated runs (Jan 10-21, ~2 runs/day √ó 5 services)

### Root Cause

Prometheus 30-day queries require sufficient historical data. Between Jan 10-21, one or more of:
1. Prometheus restarts/data loss (15-day retention limit)
2. Insufficient data age for 30-day calculations
3. Recording rule evaluation failures (though logs show no errors)

### Resolution

‚úÖ **Snapshot collection is now functional** - Manual run confirmed all queries return valid data
‚úÖ **Timer is active** - Next automated run: Jan 22, 23:52:53
‚úÖ **Future collections will succeed** - Prometheus has adequate data retention now
‚ùå **Backfill not possible** - Historical data outside 15-day retention is lost

### Data Quality for Feb 1 Calibration

**Available data points (projected by Feb 1):**
- Jan 9: 1 snapshot
- Jan 22-31: ~10 snapshots (daily)
- **Total: ~11 valid days** (vs. 22+ days desired)

**Impact:** Calibration will be based on limited dataset. Recommendation: **Extend calibration to Feb 15** to gather 23+ days of clean data (Jan 22 - Feb 14).

---

## Recovery Trajectory Analysis

### Immich Recovery (Post-Corruption Fix on Jan 11)

| Date | SLI (30d) | Error Budget | Status |
|------|-----------|--------------|--------|
| Jan 9 | 89.01% | -108.9 min | Severe corruption |
| Jan 22 | 95.66% | -42.4 min | Recovering |
| Feb 10 (projected) | >99.5% | Positive | Expected recovery |

**Recovery rate:** +6.65 percentage points over 13 days (+0.51pp/day)
**Days to target (99.9%):** ~8.4 more days ‚Üí **Jan 30 estimated**
**Error budget recovery:** Requires 30-day window to fully clear corrupt data ‚Üí **Feb 10**

### Jellyfin Trend

| Metric | 30-day | 7-day (avg) | 24-hour |
|--------|--------|-------------|---------|
| SLI | 98.45% | 98.41% | 100.0% |
| Trend | Stable | Stable | Excellent |

**Assessment:** December incidents still affecting 30-day SLI. Current performance is healthy (100% in 24h). Error budget should recover naturally by **late January**.

---

## Service Logs Review (Jan 15-22)

### Jellyfin

**Error patterns identified:**
- `AudioNormalizationTask: Failed to find LUFS value in output` (Jan 21, 13:32)
  - Impact: None (backend media processing, doesn't affect HTTP availability)
  - Frequency: Batch processing during library scan
- `MusicBrainz` network timeout (Jan 21, 13:48)
  - Impact: None (metadata enrichment, not user-facing)

**HTTP availability:** No 5xx errors detected in Traefik metrics (24h window)

### Immich

**Error patterns identified:**
- `h264_vaapi: Failed to end picture encode` (Jan 20, 01:01-01:02)
  - Impact: Thumbnail generation delays (retries with software encoding)
  - Frequency: Sporadic during video processing
  - Mitigation: Automatic fallback to CPU encoding

**HTTP availability:** ~17 failed requests / 633 total (2.7% error rate, likely related to video processing timeouts)

**Conclusion:** Internal application errors are NOT causing HTTP-level service degradation. Current 2.7% error rate is acceptable during recovery phase.

---

## Recommendations for Next Phases

### Immediate Actions (Before Phase 3)

1. **FIX BURN RATE CALCULATION** (URGENT)
   - Update `slo-recording-rules.yml` with correct burn rate formulas
   - Use `rate()` over time windows, not `avg_over_time()` of 30-day SLI
   - Reload Prometheus: `podman exec prometheus kill -SIGHUP 1`
   - Verify alerts start working for real degradation

2. **Extend Calibration Timeline**
   - Move from Feb 1 ‚Üí **Feb 15** to gather 23+ days of valid data
   - Ensures statistical significance for p95 recommendations

3. **Monitor Snapshot Collection**
   - Verify automated runs produce valid data (check Jan 23 snapshot)
   - No backfill needed - start fresh from Jan 22

### Phase 3: Calibration Analysis (Revised: Feb 15-20)

**Prerequisites:** 23+ days of valid snapshot data (Jan 22 - Feb 14)

**Process:**
1. Run `analyze-slo-trends.sh 2026-01` (will use Jan 22-31 data)
2. Run `analyze-slo-trends.sh 2026-02` (will use Feb 1-14 data)
3. Combine datasets for comprehensive p95 analysis
4. Validate or adjust targets based on recovered performance

**Expected outcomes:**
- Immich: Likely **KEEP 99.9%** target (corruption was one-time event, current trend positive)
- Jellyfin: Likely **KEEP 99.5%** target (December incidents isolated, now 100% in 24h)

### Phase 4: Remediation System Validation (Feb 15-22)

**Critical:** MUST fix burn rate calculation before validation!

**Test plan:**
1. Deploy burn rate fix
2. Verify alerts fire correctly during controlled degradation test
3. Confirm webhook routing to `slo-violation-remediation` playbook
4. Validate SLO before/after metrics in decision log
5. Document MTTR improvements vs. manual intervention

### Phase 5: Proactive Prevention System (Feb 23+)

**Capabilities to build:**
1. Error budget forecasting (7-14 day predictions)
2. Tier 3/4 alert investigation playbooks
3. Budget-based change control (freeze at <20% budget)
4. Remediation effectiveness metrics for monthly SLO reports

---

## Appendix: Data Snapshots

### Jan 9 Snapshot (Pre-Fix)
```csv
2026-01-09 10:06:10,immich,0.8901059235907152,0.999,-108.89407640928474,0
2026-01-09 10:06:10,jellyfin,0.9771058997006159,0.995,-3.578820059876816,0
```

### Jan 22 Snapshot (Current State)
```csv
2026-01-22 00:13:25,immich,0.9566415967886117,0.999,-42.35840323828784,0
2026-01-22 00:13:25,jellyfin,0.9844525658298163,0.995,-2.109486834580075,0
2026-01-22 00:13:25,authelia,0.9977627295509776,0.999,-1.2372704462283437,0
2026-01-22 00:13:25,nextcloud,0.9993668863764127,0.995,0.8733702559668758,1
2026-01-22 00:13:25,traefik,0.9999668402029379,0.9995,0.9336745409725873,1
```

### Prometheus Query Results (Jan 22, 00:13 UTC)
```promql
# 30-day SLI values
slo:jellyfin:availability:actual = 0.9845 (98.45%)
slo:immich:availability:actual = 0.9566 (95.66%)

# 24-hour HTTP performance
sum(increase(traefik_service_requests_total{exported_service="jellyfin@file"}[24h])) = 5
sum(increase(traefik_service_requests_total{exported_service="jellyfin@file",code=~"2..|3.."}[24h])) = 5 (100%)

sum(increase(traefik_service_requests_total{exported_service="immich@file"}[24h])) = 633.11
sum(increase(traefik_service_requests_total{exported_service="immich@file",code=~"2..|3.."}[24h])) = 616.11 (97.3%)

# Burn rates (REFLECTS 30-DAY HISTORY, NOT CURRENT RATE - BUG!)
burn_rate:jellyfin:availability:1h = 3.11x
burn_rate:immich:availability:1h = 43.36x
```

---

## Conclusion

The SLO framework infrastructure is **mostly operational**, but the burn rate calculation bug represents a **critical gap in the proactive alerting strategy**. Services are currently healthy and recovering from historical incidents.

**Priority 1:** Fix burn rate calculations to enable multi-window alerting
**Priority 2:** Collect 23+ days of clean snapshot data for calibration (Jan 22 - Feb 14)
**Priority 3:** Validate remediation system once burn rate alerts are functional

With these corrections, the SLO operational excellence system will provide:
- ‚úÖ Accurate short-term degradation detection (Tier 1/2 alerts)
- ‚úÖ Automated remediation with <5min MTTR
- ‚úÖ Data-driven target calibration based on actual performance
- ‚úÖ Proactive prevention through forecasting and change control

**Next Action:** Update `slo-recording-rules.yml` to fix burn rate calculation methodology.
