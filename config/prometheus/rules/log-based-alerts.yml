# Log-Based Alert Rules
# Created: 2026-01-11
# Purpose: Alert on log-derived metrics for proactive issue detection
#
# How it works:
# 1. Promtail extracts metrics from logs at ingestion time
# 2. Metrics exposed at promtail:9080/metrics with `promtail_custom_` prefix
# 3. Prometheus scrapes Promtail every 15s
# 4. Alerts fire based on rate/increase of these counters
#
# Philosophy: Catch specific errors BEFORE they cause SLO violations

groups:
  - name: log_based_alerts
    rules:
      # ========================================================================
      # ALERT 1: High Service Error Rate - DISABLED (fundamentally flawed)
      # ========================================================================
      # Problem: Generic priority <=3 matching caused false positives
      # Example: Loki INFO logs at priority 3 triggered alert (25,275 "errors")
      # Solution: Use service-specific alerts (SLO, Traefik, cAdvisor)
      # Removed: 2026-01-17 (Phase 3 - Eliminate Fragile Log-Based Metrics)
      #
      # - alert: ServiceErrorRateHigh
      #   expr: rate(promtail_custom_service_errors_total[5m]) > 0.03
      #   for: 5m
      #   labels:
      #     severity: warning
      #     category: reliability
      #   annotations:
      #     summary: "High error rate in {{ $labels.syslog_id }}"
      #     description: |
      #       Service {{ $labels.syslog_id }} logging {{ $value | humanize }} errors/sec (>10 in 5min).
      #       Check logs: journalctl --user -u {{ $labels.syslog_id }}.service --priority=err -n 20

      # ========================================================================
      # ALERT 2: Immich Thumbnail Failures
      # ========================================================================
      - alert: ImmichThumbnailFailureHigh
        expr: rate(promtail_custom_immich_thumbnail_failures_total[1h]) > 0.0014  # >5 in 1h
        for: 10m
        labels:
          severity: warning
          category: media_processing
        annotations:
          summary: "Immich thumbnail generation failing"
          description: |
            {{ $value | humanize }} thumbnail failures/sec (>5 in 1h). Usually corrupt media files.

            Find failing assets:
            journalctl --user -u immich-server.service --since "1 hour ago" | grep "AssetGenerateThumbnails.*ERROR"

      # ========================================================================
      # ALERT 3: Nextcloud Cron Stale (REDESIGNED - absence detection)
      # ========================================================================
      # Philosophy: Alert on "absence of success" not "presence of failure"
      # Uses changes() to detect if counter incremented recently
      - alert: NextcloudCronStale
        expr: changes(promtail_custom_nextcloud_cron_success_total[10m]) == 0
        for: 2m
        labels:
          severity: critical
          category: background_jobs
        annotations:
          summary: "Nextcloud cron hasn't run in 10+ minutes"
          description: |
            Nextcloud background jobs haven't succeeded in 10+ minutes.

            Expected: Runs every 5 minutes
            Possible causes: Timer stopped, container down, execution hanging

            Investigate:
              systemctl --user status nextcloud-cron.timer
              systemctl --user status nextcloud-cron.service
              podman logs nextcloud --tail 50

      # ========================================================================
      # ALERT 4: Jellyfin Transcoding Failures - DISABLED (no activity)
      # ========================================================================
      # DISABLED: No transcoding activity in 24+ hours (2026-01-16)
      # Problem: Log-based counter accumulated historical data, causing false positives
      # Solution: Disabled until transcoding becomes regular activity
      # Alternative: Monitor Jellyfin via Traefik 5xx errors if re-enabled
      #
      # - alert: JellyfinTranscodingFailureHigh
      #   expr: rate(promtail_custom_jellyfin_transcoding_failures_total[30m]) > 0.0017
      #   for: 5m

      # ========================================================================
      # ALERT 5: Authelia Auth Failure Spike - MOVED to security-alerts.yml
      # ========================================================================
      # Moved: 2026-01-17 (Phase 4 - Alert Consolidation)
      # Reason: Uses native Traefik metrics (not log-based), belongs in security category
      # Location: config/prometheus/alerts/security-alerts.yml
      #
      # - alert: AutheliaAuthFailureSpike
      #   expr: rate(traefik_service_requests_total{exported_service="authelia@file", code=~"401|403"}[5m]) > 0.03
      #   [MOVED]

      # ========================================================================
      # ALERT 6: Container Restart Loop - DISABLED (fundamentally flawed)
      # ========================================================================
      # Problem: Log-based counter matched normal "exec_died" events (2.6M false positives)
      # Proper solution: Use cAdvisor container health metrics (see container-health-alerts.yml)
      #
      # - alert: ContainerRestartLoop  # DISABLED
      #   expr: rate(promtail_custom_container_unplanned_restarts_total[10m]) > 0.005
      #   [REMOVED]

      # ========================================================================
      # ALERT 7: Promtail Metric Extraction Stale (META-MONITORING)
      # ========================================================================
      # Detects when Promtail custom metrics stop updating (silent failures)
      # Uses Immich thumbnail metric as canary (should have some activity)
      # Added: 2026-01-17 (Phase 3 - Meta-monitoring)
      - alert: PromtailMetricExtractionStale
        expr: |
          (
            absent(promtail_custom_immich_thumbnail_failures_total)
            or
            absent(promtail_custom_nextcloud_cron_success_total)
          )
        for: 30m
        labels:
          severity: warning
          category: monitoring
        annotations:
          summary: "Promtail metric extraction may have failed"
          description: |
            One or more Promtail custom metrics are missing, indicating metric extraction failure.

            Possible causes:
            1. Promtail pipeline configuration error (regex mismatch)
            2. Log rotation position file issue
            3. Journal export service down
            4. Promtail container restarted and lost position

            Check:
            - podman logs promtail --tail 100 | grep -i error
            - systemctl --user status journal-export.service
            - curl http://localhost:9080/metrics | grep promtail_custom

            Active metrics (should exist):
            - promtail_custom_immich_thumbnail_failures_total
            - promtail_custom_nextcloud_cron_success_total
