# Log-Based Alert Rules
# Created: 2026-01-11
# Purpose: Alert on log-derived metrics for proactive issue detection
#
# How it works:
# 1. Promtail extracts metrics from logs at ingestion time
# 2. Metrics exposed at promtail:9080/metrics with `promtail_custom_` prefix
# 3. Prometheus scrapes Promtail every 15s
# 4. Alerts fire based on rate/increase of these counters
#
# Philosophy: Catch specific errors BEFORE they cause SLO violations

groups:
  - name: log_based_alerts
    rules:
      # ========================================================================
      # ALERT 1: High Service Error Rate
      # ========================================================================
      - alert: ServiceErrorRateHigh
        expr: rate(promtail_custom_service_errors_total[5m]) > 0.03  # >10 errors in 5min
        for: 5m
        labels:
          severity: warning
          category: reliability
        annotations:
          summary: "High error rate in {{ $labels.syslog_id }}"
          description: |
            Service {{ $labels.syslog_id }} logging {{ $value | humanize }} errors/sec (>10 in 5min).

            Check logs: journalctl --user -u {{ $labels.syslog_id }}.service --priority=err -n 20

      # ========================================================================
      # ALERT 2: Immich Thumbnail Failures
      # ========================================================================
      - alert: ImmichThumbnailFailureHigh
        expr: rate(promtail_custom_immich_thumbnail_failures_total[1h]) > 0.0014  # >5 in 1h
        for: 10m
        labels:
          severity: warning
          category: media_processing
        annotations:
          summary: "Immich thumbnail generation failing"
          description: |
            {{ $value | humanize }} thumbnail failures/sec (>5 in 1h). Usually corrupt media files.

            Find failing assets:
            journalctl --user -u immich-server.service --since "1 hour ago" | grep "AssetGenerateThumbnails.*ERROR"

      # ========================================================================
      # ALERT 3: Nextcloud Cron Stale (REDESIGNED - absence detection)
      # ========================================================================
      # Philosophy: Alert on "absence of success" not "presence of failure"
      # Uses changes() to detect if counter incremented recently
      - alert: NextcloudCronStale
        expr: changes(promtail_custom_nextcloud_cron_success_total[10m]) == 0
        for: 2m
        labels:
          severity: critical
          category: background_jobs
        annotations:
          summary: "Nextcloud cron hasn't run in 10+ minutes"
          description: |
            Nextcloud background jobs haven't succeeded in 10+ minutes.

            Expected: Runs every 5 minutes
            Possible causes: Timer stopped, container down, execution hanging

            Investigate:
              systemctl --user status nextcloud-cron.timer
              systemctl --user status nextcloud-cron.service
              podman logs nextcloud --tail 50

      # ========================================================================
      # ALERT 4: Jellyfin Transcoding Failures - DISABLED (no activity)
      # ========================================================================
      # DISABLED: No transcoding activity in 24+ hours (2026-01-16)
      # Problem: Log-based counter accumulated historical data, causing false positives
      # Solution: Disabled until transcoding becomes regular activity
      # Alternative: Monitor Jellyfin via Traefik 5xx errors if re-enabled
      #
      # - alert: JellyfinTranscodingFailureHigh
      #   expr: rate(promtail_custom_jellyfin_transcoding_failures_total[30m]) > 0.0017
      #   for: 5m

      # ========================================================================
      # ALERT 5: Authelia Auth Failure Spike (REDESIGNED - Traefik metrics)
      # ========================================================================
      # Changed from log-based to Traefik HTTP status codes (2026-01-16)
      # Reason: Counter accumulation from log rotation caused false positives
      # Benefit: Native metrics, not affected by log rotation, more reliable
      - alert: AutheliaAuthFailureSpike
        expr: rate(traefik_service_requests_total{exported_service="authelia@file", code=~"401|403"}[5m]) > 0.03  # >10 in 5min
        for: 2m
        labels:
          severity: warning
          category: security
        annotations:
          summary: "High authentication failure rate"
          description: |
            {{ $value | humanize }} auth failures/sec (HTTP 401/403 from Traefik).

            401 = User not authenticated (not logged in)
            403 = Authentication failed or IP banned

            Check CrowdSec: podman exec crowdsec cscli decisions list
            Check Authelia logs: podman logs authelia --tail 50

      # ========================================================================
      # ALERT 6: Container Restart Loop - DISABLED (fundamentally flawed)
      # ========================================================================
      # Problem: Log-based counter matched normal "exec_died" events (2.6M false positives)
      # Proper solution: Use cAdvisor container health metrics (see container-health-alerts.yml)
      #
      # - alert: ContainerRestartLoop  # DISABLED
      #   expr: rate(promtail_custom_container_unplanned_restarts_total[10m]) > 0.005
      #   [REMOVED]
